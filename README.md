# ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition

We introduce ResearchBench, an LLM-based benchmark and automated framework designed to evaluate and enhance the scientific discovery ability of large language models (LLMs). ResearchBench decomposes the hypothesis formulation process into three fundamental sub-tasks: inspiration retrieval, hypothesis composition, and hypothesis ranking. Our framework demonstrates that LLMs can excel at retrieving out-of-distribution inspirations and composing high-quality hypotheses. This positions LLMs as powerful "research hypothesis mines" for automated scientific discovery across diverse disciplines.

In the `dataset` folder, we provide the decomposition results from our automated framework for 678 papers across 12 disciplines.

In the `code` folder, we provide code for Inspiration Retrieval, Hypothesis Composition and Hypothesis Ranking task.
